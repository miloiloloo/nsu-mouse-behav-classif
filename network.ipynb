{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network\n",
    "\n",
    "Copyright (c) 2017 Alexander Menkin\n",
    "\n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file at\n",
    "https://github.com/miloiloloo/diploma_2017_method/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, UpSampling2D,core\n",
    "from keras.layers import BatchNormalization, Activation, Reshape, LeakyReLU, Dropout, Flatten,Cropping2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, merge\n",
    "from keras.engine import InputSpec\n",
    "from keras import activations, initializations, regularizers, constraints,callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import sklearn.metrics\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_learning_data(file_path):\n",
    "    ''' Load data from learning data file and check it\n",
    "        It returns (assemblies, probabilities)\n",
    "    '''\n",
    "    \n",
    "    ''' Load data from learning data file '''\n",
    "    try:\n",
    "        f = open(file_path, 'rb')\n",
    "        save = pickle.load(f)\n",
    "        assemblies = save['assemblies']\n",
    "        probabilities = save['probabilities']\n",
    "        f.close()\n",
    "        del save\n",
    "    except Exception as e:\n",
    "        print(\"Unable to load the file: \" + file_path)\n",
    "        raise\n",
    "        \n",
    "    ''' Check learning data '''\n",
    "    if assemblies.shape[0] != probabilities.shape[0]:\n",
    "        print(\"Incorrect sizes: assemblies (\" + str(assemblies.shape[0]) + \") and probabilities (\" + str(probabilities.shape[0]) + \"). They must be equal\")\n",
    "        raise Exception\n",
    "    \n",
    "    return (assemblies, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_valid_test_data(assemblies, probabilities, train_size, valid_size, test_size):\n",
    "    ''' Separate learning data to train, valid and test data\n",
    "        It returns (train_assemblies, train_probabilities, valid_assemblies, valid_probabilities, test_assemblies, test_probabilities)\n",
    "    '''\n",
    "    \n",
    "    ''' CHECK INPUT '''\n",
    "    ''' Incorrect input 1: incorrect sizes of assemblies and probabilities '''\n",
    "    if assemblies.shape[0] != probabilities.shape[0]:\n",
    "        raise Exception\n",
    "    ''' Incorrect input 2: size of train/valid/test sets can't be negative '''\n",
    "    if train_size < 0 or valid_size < 0 or test_size < 0:\n",
    "        raise Exception\n",
    "    ''' Incorrect input 3: incorrect sizes of train, valid and test_size'''\n",
    "    if train_size + valid_size + test_size > assemblies.shape[0]:\n",
    "        raise Exception\n",
    "    \n",
    "    ''' ALGORITHM & OUTPUT '''\n",
    "    ''' Assemblies and indecies permutation '''\n",
    "    permutation_indecies = np.random.permutation(assemblies.shape[0])\n",
    "    assemblies = assemblies[permutation_indecies, :, :, :]\n",
    "    probabilities = probabilities[permutation_indecies, :]\n",
    "    \n",
    "    ''' Train set '''\n",
    "    train_assemblies = assemblies[0 : train_size, :, :, :]\n",
    "    train_probabilities = probabilities[0 : train_size, :] \n",
    "    ''' Valid set '''\n",
    "    valid_assemblies = assemblies[train_size : train_size + valid_size, :, :, :]\n",
    "    valid_probabilities = probabilities[train_size : train_size + valid_size, :]\n",
    "    ''' Test set '''\n",
    "    test_assemblies = assemblies[train_size + valid_size : train_size + valid_size + test_size, :, :, :]\n",
    "    test_probabilities = probabilities[train_size + valid_size : train_size + valid_size + test_size, :]\n",
    "    \n",
    "    return (train_assemblies, train_probabilities, valid_assemblies, valid_probabilities, test_assemblies, test_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probabilities_stat(probabilities):\n",
    "    ''' Print probabilities mean '''\n",
    "    \n",
    "    for class_idx in range (0, probabilities.shape[1]):\n",
    "        print(\"class #\" + str(class_idx) + \":\\t\" + str(np.mean(probabilities[:, class_idx])))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_probabilities(probabilities):\n",
    "    ''' Get labels from probabilities '''\n",
    "    \n",
    "    labels = np.zeros(shape=probabilities.shape)\n",
    "    for idx in range(0, probabilities.shape[0]):\n",
    "        labels[idx, np.argmax(probabilities[idx, :])] = 1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(train_set, valid_set, test_set):\n",
    "    ''' Normalize sets '''\n",
    "    \n",
    "    ''' Count mean and max '''\n",
    "    train_mean = np.mean(train_set)\n",
    "    train_max = np.max(train_set)\n",
    "    \n",
    "    ''' Normalize '''\n",
    "    train_set = (train_set - train_mean) / train_max\n",
    "    valid_set = (valid_set - train_mean) / train_max\n",
    "    test_set = (test_set - train_mean) / train_max\n",
    "    \n",
    "    ''' OUTPUT '''\n",
    "    return (train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unite_learning_data(tuple_of_assemblies, tuple_of_probabilities):\n",
    "    return (np.concatenate(tuple_of_assemblies, axis=0), np.concatenate(tuple_of_probabilities, axis=0))\n",
    "\n",
    "def permutate_learning_data(assemblies, probabilities):\n",
    "    permutation_indecies = np.random.permutation(assemblies.shape[0])\n",
    "    assemblies = assemblies[permutation_indecies, :, :, :]\n",
    "    probabilities = probabilities[permutation_indecies, :]\n",
    "    return (assemblies, probabilities)\n",
    "\n",
    "def get_learning_data_of_experiment(patch_size, offset, number_of_neighbours_per_side, number_of_experiment):\n",
    "    array_of_assemblies = []\n",
    "    array_of_probabilities = []\n",
    "    # Warning: do not forget write dir path\n",
    "    input_directory_path = \"./learning_data/size_\" + str(patch_size) + \"_offset_\" + str(offset) + \"_left_\" + str(number_of_neighbours_per_side) + \"_right_\" + str(number_of_neighbours_per_side) + \"/\"\n",
    "    part_idx = 1\n",
    "    try:\n",
    "        while True:\n",
    "            input_learning_data_file_path = input_directory_path + str(number_of_experiment) + \"_\" + str(part_idx) + \".pickle\"\n",
    "            assemblies, probabilities = load_learning_data(input_learning_data_file_path)\n",
    "            print(input_learning_data_file_path)\n",
    "            array_of_assemblies = array_of_assemblies + [assemblies]\n",
    "            array_of_probabilities = array_of_probabilities + [probabilities]\n",
    "            part_idx += 1\n",
    "    except:\n",
    "        print(\"\\n\")\n",
    "    if len(array_of_assemblies) == 0 or len(array_of_probabilities) == 0:\n",
    "        raise Exception\n",
    "    return unite_learning_data(tuple(array_of_assemblies), tuple(array_of_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mygenerator:\n",
    "    def __init__(self, assemblies, probabilities, batch_size, min_stretch_k, max_stretch_k):\n",
    "        \n",
    "        ''' Check input '''\n",
    "        if assemblies.shape[0] == 0:\n",
    "            raise Exception\n",
    "        if assemblies.shape[0] != probabilities.shape[0]:\n",
    "            raise Exception\n",
    "        if batch_size <= 0:\n",
    "            raise Exception\n",
    "        if min_stretch_k <= 0:\n",
    "            raise Exception\n",
    "        if max_stretch_k <= 0:\n",
    "            raise Exception\n",
    "        \n",
    "        ''' Predefined parameters '''\n",
    "        self._NOISE_AMPLITUDE = 0.005\n",
    "        \n",
    "        ''' Init '''\n",
    "        self._idx = 0\n",
    "        self._assemblies = assemblies\n",
    "        self._probabilities = probabilities\n",
    "        self._batch_size = batch_size\n",
    "        self._min_stretch_k = min_stretch_k\n",
    "        self._max_stretch_k = max_stretch_k\n",
    "        self._permutation = np.random.permutation(self._assemblies.shape[0])\n",
    "        \n",
    "    def generate(self):\n",
    "        \n",
    "        result_assemblies = np.random.rand(\n",
    "            self._batch_size,\n",
    "            self._assemblies.shape[1],\n",
    "            self._assemblies.shape[2],\n",
    "            self._assemblies.shape[3]\n",
    "        )\n",
    "        result_assemblies = self._NOISE_AMPLITUDE * result_assemblies\n",
    "        result_probabilities = np.ndarray(\n",
    "            shape=(self._batch_size, self._probabilities.shape[1])\n",
    "        )\n",
    "        \n",
    "        for batch_idx in range(0, self._batch_size):\n",
    "            \n",
    "            ''' Init probability '''\n",
    "            result_probabilities[batch_idx, :] = self._probabilities[self._permutation[self._idx], :]\n",
    "            \n",
    "            ''' Count stretch k '''\n",
    "            stretch_k_x = random.uniform(self._min_stretch_k, self._max_stretch_k)\n",
    "            stretch_k_y = random.uniform(self._min_stretch_k, self._max_stretch_k)\n",
    "\n",
    "            ''' '''\n",
    "            for patch_idx in range(0, self._assemblies.shape[3]):\n",
    "                \n",
    "                zoom_patch = ndimage.zoom(self._assemblies[self._permutation[self._idx], :, :, patch_idx], [stretch_k_x, stretch_k_y])\n",
    "                \n",
    "                offset_x = (self._assemblies.shape[1] - zoom_patch.shape[0])/2\n",
    "                offset_y = (self._assemblies.shape[2] - zoom_patch.shape[1])/2\n",
    "                \n",
    "                from_x = 0\n",
    "                from_y = 0\n",
    "                to_x = 0\n",
    "                to_y = 0\n",
    "                                \n",
    "                if zoom_patch.shape[0] <= self._assemblies.shape[1]:\n",
    "                    from_x = 0\n",
    "                    to_x = zoom_patch.shape[0]\n",
    "                else:\n",
    "                    from_x = -offset_x\n",
    "                    to_x = from_x + self._assemblies.shape[1]\n",
    "                if zoom_patch.shape[1] <= self._assemblies.shape[2]:\n",
    "                    from_y = 0\n",
    "                    to_y = zoom_patch.shape[1]\n",
    "                else:\n",
    "                    from_y = -offset_y\n",
    "                    to_y = from_y + self._assemblies.shape[2]\n",
    "                \n",
    "                ''' Init assembly '''\n",
    "                result_assemblies[\n",
    "                    batch_idx,\n",
    "                    from_x + offset_x : to_x + offset_x,\n",
    "                    from_y + offset_y : to_y + offset_y,\n",
    "                    patch_idx\n",
    "                ] = zoom_patch[\n",
    "                    from_x : to_x,\n",
    "                    from_y : to_y\n",
    "                ]\n",
    "            \n",
    "            die = random.choice([0, 1])\n",
    "            if die == 1:\n",
    "                ''' T '''\n",
    "                result_assemblies[batch_idx, :, :, :] = np.flipud(result_assemblies[batch_idx, :, :, :])\n",
    "                        \n",
    "            if self._idx == (self._assemblies.shape[0] - 1):\n",
    "                self._permutation = np.random.permutation(self._assemblies.shape[0])\n",
    "            self._idx = (self._idx + 1) % self._assemblies.shape[0]\n",
    "        \n",
    "        return (result_assemblies, result_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load data '''\n",
    "patch_size = 64\n",
    "offset = 2\n",
    "number_of_neighbours_per_side = 1\n",
    "\n",
    "print(\"\\n\")\n",
    "assemblies1, probabilities1 = get_learning_data_of_experiment(patch_size, offset, number_of_neighbours_per_side, 1)\n",
    "assemblies2, probabilities2 = get_learning_data_of_experiment(patch_size, offset, number_of_neighbours_per_side, 2)\n",
    "assemblies3, probabilities3 = get_learning_data_of_experiment(patch_size, offset, number_of_neighbours_per_side, 3)\n",
    "assemblies4, probabilities4 = get_learning_data_of_experiment(patch_size, offset, number_of_neighbours_per_side, 4)\n",
    "\n",
    "\n",
    "a = (assemblies1, assemblies2, assemblies3, assemblies4)\n",
    "p = (probabilities1, probabilities2, probabilities3, probabilities4)\n",
    "tr_a = []\n",
    "tr_p = []\n",
    "v_a = []\n",
    "v_p = []\n",
    "te_a = []\n",
    "te_p = []\n",
    "\n",
    "for i in range(0, 4):\n",
    "    assemblies = a[i]\n",
    "    probabilities = p[i]\n",
    "    dev1 = int(0.75*assemblies.shape[0])\n",
    "    dev2 = dev1 + int(0.125*assemblies.shape[0])\n",
    "    \n",
    "    tr_a = tr_a + [assemblies[0:dev1,:,:,:]]\n",
    "    tr_p = tr_p + [probabilities[0:dev1,:]]\n",
    "    \n",
    "    v_a = v_a + [assemblies[dev1:dev2,:,:,:]]\n",
    "    v_p = v_p + [probabilities[dev1:dev2,:]]\n",
    "    \n",
    "    te_a = te_a + [assemblies[dev2:assemblies.shape[0],:,:,:]]\n",
    "    te_p = te_p + [probabilities[dev2:assemblies.shape[0],:]]\n",
    "\n",
    "train_assemblies, train_probabilities = unite_learning_data(tr_a, tr_p)\n",
    "valid_assemblies, valid_probabilities = unite_learning_data(v_a, v_p)\n",
    "test_assemblies, test_probabilities = unite_learning_data(te_a, te_p)\n",
    "\n",
    "permutation_indecies = np.random.permutation(train_assemblies.shape[0])\n",
    "train_assemblies = train_assemblies[permutation_indecies, :, :, :]\n",
    "train_probabilities = train_probabilities[permutation_indecies, :]\n",
    "train_assemblies = np.clip(train_assemblies, 0, 0.1)\n",
    "\n",
    "permutation_indecies = np.random.permutation(valid_assemblies.shape[0])\n",
    "valid_assemblies = valid_assemblies[permutation_indecies, :, :, :]\n",
    "valid_probabilities = valid_probabilities[permutation_indecies, :]\n",
    "valid_assemblies = np.clip(valid_assemblies, 0, 0.1)\n",
    "\n",
    "permutation_indecies = np.random.permutation(test_assemblies.shape[0])\n",
    "test_assemblies = test_assemblies[permutation_indecies, :, :, :]\n",
    "test_probabilities = test_probabilities[permutation_indecies, :]\n",
    "test_assemblies = np.clip(test_assemblies, 0, 0.1)\n",
    "\n",
    "train_set_size = train_assemblies.shape[0]\n",
    "valid_set_size = valid_assemblies.shape[0]\n",
    "test_set_size = test_assemblies.shape[0]\n",
    "all_sets_size = train_set_size + valid_set_size + test_set_size\n",
    "\n",
    "\n",
    "print(\"\\nSets\")\n",
    "print(\"-------\")\n",
    "print(\"All set size:\\t\" + str(all_sets_size))\n",
    "print(\"Train size:\\t\" + str(train_set_size))\n",
    "print(\"Valid set size:\\t\" + str(valid_set_size))\n",
    "print(\"Test set size:\\t\" + str(test_set_size))\n",
    "print(\"-------\\n\")\n",
    "print(\"Train probabilities stat\")\n",
    "print(\"-------\")\n",
    "print_probabilities_stat(train_probabilities)\n",
    "print(\"-------\\n\")\n",
    "print(\"Valid probabilities stat\")\n",
    "print(\"-------\")\n",
    "print_probabilities_stat(valid_probabilities)\n",
    "print(\"-------\\n\")\n",
    "print(\"Test probabilities stat\")\n",
    "print(\"-------\")\n",
    "print_probabilities_stat(test_probabilities)\n",
    "print(\"-------\\n\")\n",
    "\n",
    "''' Normalization '''\n",
    "train_assemblies, valid_assemblies, test_assemblies = normalization(\n",
    "    train_set = train_assemblies,\n",
    "    valid_set = valid_assemblies,\n",
    "    test_set = test_assemblies\n",
    ")\n",
    "\n",
    "''' Balancing '''\n",
    "''' Get train probabilities stat '''\n",
    "train_probabilities_stat = np.zeros(shape=(train_probabilities.shape[1]), dtype=train_probabilities.dtype)\n",
    "for class_idx in range(0, train_probabilities.shape[1]):\n",
    "    train_probabilities_stat[class_idx] = np.mean(train_probabilities[:, class_idx])\n",
    "''' Balance '''\n",
    "for train_idx in range(0, train_probabilities.shape[0]):\n",
    "    class_idx = np.argmax(train_probabilities[train_idx, :])\n",
    "    train_probabilities[train_idx, :] = np.zeros(shape=(train_probabilities.shape[1]))\n",
    "    train_probabilities[train_idx, class_idx] = 1/(1 + train_probabilities_stat[class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly_size = number_of_neighbours_per_side * 2 + 1\n",
    "\n",
    "conv_nb_filters = None\n",
    "if assembly_size < 8:    \n",
    "    conv_nb_filters = [8, 16, 32, 64]\n",
    "if assembly_size >= 8 and assembly_size < 16:\n",
    "    conv_nb_filters = [16, 32, 48, 64]\n",
    "if assembly_size >= 16 and assembly_size < 32:\n",
    "    conv_nb_filters = [32, 44, 54, 64]\n",
    "if assembly_size >= 32:\n",
    "    raise Exception\n",
    "\n",
    "    \n",
    "input_ = Input((64, 64, assembly_size), name='input')\n",
    "\n",
    "\n",
    "conv2d_1 = Convolution2D(nb_filter=conv_nb_filters[0],\n",
    "                         nb_row=4,\n",
    "                         nb_col=4,\n",
    "                         activation='relu',\n",
    "                         border_mode='same',\n",
    "                         name='conv2d_1')(input_)\n",
    "maxpool2d_1 = MaxPooling2D(pool_size=(2, 2), name='max_pool2d_1')(conv2d_1)\n",
    "dropout_1 = Dropout(0.1, name='dropout_1')(maxpool2d_1)\n",
    "\n",
    "\n",
    "conv2d_2 = Convolution2D(nb_filter=conv_nb_filters[1],\n",
    "                         nb_row=4,\n",
    "                         nb_col=4,\n",
    "                         activation='relu',\n",
    "                         border_mode='same',\n",
    "                         name='conv_2d_2')(dropout_1)\n",
    "maxpool2d_2 = MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_2')(conv2d_2)\n",
    "dropout_2 = Dropout(0.1, name='dropout_2')(maxpool2d_2)\n",
    "\n",
    "    \n",
    "conv2d_3 = Convolution2D(nb_filter=conv_nb_filters[2],\n",
    "                         nb_row=4,\n",
    "                         nb_col=4,\n",
    "                         activation='relu',\n",
    "                         border_mode='same',\n",
    "                         name='conv_2d_3')(dropout_2)\n",
    "maxpool2d_3 = MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_3')(conv2d_3)\n",
    "dropout_3 = Dropout(0.1, name='dropout_3')(maxpool2d_3)\n",
    "\n",
    "\n",
    "conv2d_4 = Convolution2D(nb_filter=conv_nb_filters[3],\n",
    "                         nb_row=4,\n",
    "                         nb_col=4,\n",
    "                         activation='relu',\n",
    "                         border_mode='same',\n",
    "                         name='conv_2d_4')(dropout_3)\n",
    "maxpool2d_4 = MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_4')(conv2d_4)\n",
    "\n",
    "\n",
    "reshape_1 = Reshape((4 * 4 * 64,), name='reshape_1')(maxpool2d_4)\n",
    "\n",
    "\n",
    "dense_1 = Dense(output_dim=64, activation='relu', name='dense_1')(reshape_1)\n",
    "\n",
    "\n",
    "dense_2 = Dense(output_dim=5, name='dense_2')(dense_1)\n",
    "\n",
    "\n",
    "activation_1 = Activation('softmax', name='activation_1')(dense_2)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input=input_, output=activation_1)\n",
    "\n",
    "model_for_tsne = Model(input=input_, output=dense_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_tsne.compile(optimizer='adam',\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "model_for_tsne.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return self.it.next()\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "my_generator = mygenerator(train_assemblies, train_probabilities, 32, 0.9, 1.1)\n",
    "\n",
    "@threadsafe_generator\n",
    "def generate():\n",
    "    while True:\n",
    "        [data,classes] = my_generator.generate()\n",
    "        yield (data, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session() \n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train model '''\n",
    "model.fit_generator(\n",
    "    generator=generate(),\n",
    "    samples_per_epoch=6400,\n",
    "    nb_epoch = 10,\n",
    "    validation_data=(valid_assemblies,valid_probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_stat(probabilities, predicted_probabilities):\n",
    "    \n",
    "    ''' Check input '''\n",
    "    if probabilities.shape[0] != predicted_probabilities.shape[0]:\n",
    "        raise Exception\n",
    "    if probabilities.shape[1] != predicted_probabilities.shape[1]:\n",
    "        raise Exception\n",
    "    \n",
    "    ''' Go to classes '''\n",
    "    for class_idx in range(0, probabilities.shape[1]):\n",
    "        \n",
    "        print(\"class #\" + str(class_idx))\n",
    "        \n",
    "        TP = 0.0\n",
    "        TN = 0.0\n",
    "        FP = 0.0\n",
    "        FN = 0.0\n",
    "        for idx in range(0, probabilities.shape[0]):\n",
    "            if np.argmax(predicted_probabilities[idx, :]) == class_idx:\n",
    "                if np.argmax(probabilities[idx, :]) == class_idx:\n",
    "                    TP += 1.0\n",
    "                else:\n",
    "                    FP += 1.0\n",
    "            else:\n",
    "                if np.argmax(probabilities[idx, :]) == class_idx:\n",
    "                    FN += 1.0\n",
    "                else:\n",
    "                    TN += 1.0\n",
    "        \n",
    "        if TP + FN != 0:\n",
    "            TPR = TP/(TP + FN)\n",
    "        else:\n",
    "            print(\"NO TPR, FNR, BM\")\n",
    "            TPR = 0\n",
    "        \n",
    "        if TN + FP != 0:\n",
    "            TNR = TN/(TN + FP)\n",
    "        else:\n",
    "            print(\"NO TNR, FPR, BM\")\n",
    "            TNR = 0\n",
    "        \n",
    "        if TP + FP != 0:\n",
    "            PPV = TP/(TP + FP)\n",
    "        else:\n",
    "            print(\"NO PPV, FDR, MK\")\n",
    "            PPV = 0\n",
    "        \n",
    "        if TN + FN != 0:\n",
    "            NPV = TN/(TN + FN)\n",
    "        else:\n",
    "            print(\"NO NPV, FOR, MK\")\n",
    "            NPV = 0\n",
    "        \n",
    "        FNR = 1 - TPR\n",
    "        FPR = 1 - TNR\n",
    "        FDR = 1 - PPV\n",
    "        FOR = 1 - NPV\n",
    "        ACC = (TP + TN)/(TP + FN + TN + FP)\n",
    "        F_1 = (2*TP)/(2*TP + FP + FN)\n",
    "        \n",
    "        if TP + FP != 0 and TP + FN != 0 and TN + FP != 0 and TN + FN != 0:\n",
    "            MCC = (TP*TN - FP*FN)/(math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)))\n",
    "        else:\n",
    "            print(\"NO MCC\") \n",
    "            MCC = 0\n",
    "        \n",
    "        BM = TPR + TNR - 1\n",
    "        MK = PPV + NPV - 1\n",
    "        \n",
    "        print(\"-------\")\n",
    "        print(\"TP:\\t\" + str(TP))\n",
    "        print(\"TN:\\t\" + str(TN))\n",
    "        print(\"FP:\\t\" + str(FP))\n",
    "        print(\"FN:\\t\" + str(FN))\n",
    "        print(\"TPR:\\t\" + str(TPR))\n",
    "        print(\"TNR:\\t\" + str(TNR))\n",
    "        print(\"PPV:\\t\" + str(PPV))\n",
    "        print(\"NPV:\\t\" + str(NPV))\n",
    "        print(\"FNR:\\t\" + str(FNR))\n",
    "        print(\"FPR:\\t\" + str(FPR))\n",
    "        print(\"FDR:\\t\" + str(FDR))\n",
    "        print(\"FOR:\\t\" + str(FOR))\n",
    "        print(\"ACC:\\t\" + str(ACC))\n",
    "        print(\"F_1:\\t\" + str(F_1))\n",
    "        print(\"MCC:\\t\" + str(MCC))\n",
    "        print(\"BM:\\t\" + str(BM))\n",
    "        print(\"MK:\\t\" + str(MK))\n",
    "        print(\"-------\\n\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_stat_table(probabilities, predicted_probabilities):\n",
    "    \n",
    "    ''' Check input '''\n",
    "    if probabilities.shape[0] != predicted_probabilities.shape[0]:\n",
    "        raise Exception\n",
    "    if probabilities.shape[1] != predicted_probabilities.shape[1]:\n",
    "        raise Exception\n",
    "        \n",
    "    class_stat_table = np.zeros(shape=(probabilities.shape[1], probabilities.shape[1]), dtype=np.float32)\n",
    "    for idx in range(0, probabilities.shape[0]):\n",
    "        first_idx = np.argmax(probabilities[idx, :])\n",
    "        second_idx = np.argmax(predicted_probabilities[idx, :])\n",
    "        class_stat_table[first_idx, second_idx] += 1\n",
    "    for first_idx in range(0, probabilities.shape[1]):\n",
    "        class_stat_table[first_idx, :] = class_stat_table[first_idx, :] / np.sum(class_stat_table[first_idx, :])\n",
    "    return class_stat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Test model '''\n",
    "test_score = model.evaluate(test_assemblies, test_probabilities, batch_size=16)\n",
    "\n",
    "''' Print model's test stat '''\n",
    "print(\"\\n\\nTest model stat\")\n",
    "print(\"-------\")\n",
    "print(\"Loss:\\t\\t\" + str(test_score[0]))\n",
    "print(\"Accuracy:\\t\" + str(test_score[1]))\n",
    "print(\"-------\\n\")\n",
    "\n",
    "test_predicted_probabilities = model.predict(test_assemblies)\n",
    "\n",
    "print_confusion_stat(\n",
    "    probabilities=test_probabilities,\n",
    "    predicted_probabilities = test_predicted_probabilities\n",
    ")\n",
    "\n",
    "cst = show_stat_table(\n",
    "    probabilities=test_probabilities,\n",
    "    predicted_probabilities = test_predicted_probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Purples):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(np.round(cst,2), classes=['UNDEF', 'LOCOM', 'IMMOB', 'REAR', 'GROOM'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = 0\n",
    "tsne = TSNE(n_components=2, n_iter=1000, random_state=RS, metric='euclidean')\n",
    "X_t = tsne.fit_transform(model_for_tsne.predict(test_assemblies))\n",
    "y = np.zeros(shape=(test_probabilities.shape[0]))\n",
    "for ind in range(0, test_probabilities.shape[0]):\n",
    "    y[ind] = np.argmax(test_probabilities[ind,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "#plt.scatter(X_t[np.where(y == 0), 0],X_t[np.where(y == 0), 1],marker='o', color='r',linewidth='1', label='undef')  \n",
    "plt.scatter(X_t[np.where(y == 1), 0],X_t[np.where(y == 1), 1],marker='o', color='g',linewidth='0.05', label='locom')\n",
    "plt.scatter(X_t[np.where(y == 2), 0],X_t[np.where(y == 2), 1],marker='o', color='b',linewidth='0.05', label='immob')\n",
    "plt.scatter(X_t[np.where(y == 3), 0],X_t[np.where(y == 3), 1],marker='o', color='c',linewidth='0.05', label='rear')\n",
    "plt.scatter(X_t[np.where(y == 4), 0],X_t[np.where(y == 4), 1],marker='o', color='m',linewidth='0.05', label='groom')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
