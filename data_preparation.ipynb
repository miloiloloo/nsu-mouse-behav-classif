{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "Copyright (c) 2017 Alexander Menkin\n",
    "\n",
    "Use of this source code is governed by an MIT-style license that can be found in the LICENSE file at\n",
    "https://github.com/miloiloloo/diploma_2017_method/blob/master/LICENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_depth_data(filepath):\n",
    "    try:\n",
    "        f = open(filepath, 'rb')\n",
    "        save = pickle.load(f)\n",
    "        number_of_frames = save['number_of_frames']\n",
    "        assembly_offsets = save['assembly_offsets']\n",
    "        assemblies = save['assemblies']\n",
    "        indecies = save['indecies'] \n",
    "        f.close()\n",
    "        del save\n",
    "    except Exception as e:\n",
    "        raise\n",
    "    return (number_of_frames, assembly_offsets, assemblies, indecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_laboras_data(filepath):\n",
    "    try:\n",
    "        f = open(filepath, 'rb')\n",
    "        save = pickle.load(f)\n",
    "        time_intervals = save['time_intervals']\n",
    "        probabilities = save['probabilities']\n",
    "        f.close()\n",
    "        del save\n",
    "    except Exception as e:\n",
    "        raise\n",
    "    return (time_intervals, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_for_learning(file_path, assemblies, probabilities):\n",
    "    try:\n",
    "        f = open(file_path, 'wb')\n",
    "        save = {\n",
    "            'assemblies': assemblies,\n",
    "            'probabilities': probabilities,\n",
    "        }\n",
    "        pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', file_path, ':', e)\n",
    "        raise\n",
    "    print('Data has been saved')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_data(start_time, stop_time,\n",
    "                      time_intervals, probabilities,\n",
    "                      number_of_frames,\n",
    "                      indecies, assembly_offsets, assemblies):\n",
    "    ''' Combine processed laboras data and assemblies data from depth sensor, where:\n",
    "        It returns a tuple (data, probabilities)\n",
    "        Remember: all time is in seconds!!!\n",
    "    '''\n",
    "    \n",
    "    ''' CHECK INPUT '''\n",
    "    ''' Incorrect input 1: start time more then stop time '''\n",
    "    if start_time > stop_time:\n",
    "        raise Exception\n",
    "    ''' Incorrect input 2: incorrect time intervals '''\n",
    "    for time_interval_idx in range(0, time_intervals.shape[0]):\n",
    "        if time_interval_idx > 0:\n",
    "            if time_intervals[time_interval_idx, 0] < time_intervals[time_interval_idx - 1, 1]:\n",
    "                raise Exception\n",
    "        if time_intervals[time_interval_idx, 0] > time_intervals[time_interval_idx, 1]:\n",
    "            raise Exception\n",
    "    ''' Incorrect input 3: incorrect probabilities '''\n",
    "    if np.min(probabilities) < 0:\n",
    "        raise Exception\n",
    "    ''' Incorrect input 4: number of indecies is not equal number of assemblies '''\n",
    "    if indecies.shape[0] != assemblies.shape[0]:\n",
    "        raise Exception\n",
    "    ''' Incorrect input 5: incorrect combination of number of frames, indecies and assembly_offsets '''\n",
    "    \n",
    "    \n",
    "    ''' ALGORITHM '''\n",
    "    ''' Predefined undef class index and undef probability limit'''\n",
    "    undef_class_idx = 0\n",
    "    undef_probability_limit = 0.5\n",
    "    \n",
    "    ''' Duration of the experiment '''\n",
    "    duration = stop_time - start_time\n",
    "    \n",
    "    ''' Time per frame under conditions:\n",
    "        1. Frist frame (0) is matched to the start_time;\n",
    "        2. Last frame (number_of_frames - 1) is matched to the stop_time.\n",
    "    ''' \n",
    "    time_per_frame = float(duration) / (number_of_frames - 1)\n",
    "        \n",
    "    ''' Containers for learning data '''\n",
    "    learning_assemblies = np.ndarray(shape=assemblies.shape, dtype=assemblies.dtype)\n",
    "    learning_probabilities = np.ndarray(shape=(assemblies.shape[0], probabilities.shape[1]), dtype=probabilities.dtype)\n",
    "    learning_data_counter = 0\n",
    "    \n",
    "    ''' ALGORITHM '''\n",
    "    for assembly_idx in range(0, assemblies.shape[0]):\n",
    "        \n",
    "        ''' Prepare data for assembly '''\n",
    "        is_assembly_acceptable = True\n",
    "        assembly_probabilities = np.zeros(shape=(probabilities.shape[1]), dtype=probabilities.dtype)\n",
    "        patch_frame_indecies = assembly_offsets + indecies[assembly_idx]\n",
    "        \n",
    "        ''' Count assembly_probabilities '''\n",
    "        for patch_frame_idx in patch_frame_indecies:\n",
    "            \n",
    "            ''' Time for patch's frame '''\n",
    "            time_of_frame = start_time + time_per_frame*patch_frame_idx\n",
    "            \n",
    "            ''' Find time interval '''\n",
    "            time_interval_has_been_found = False\n",
    "            time_interval_for_patch_idx = 0\n",
    "            for time_interval_idx in range(0, time_intervals.shape[0]):\n",
    "                if time_of_frame >= time_intervals[time_interval_idx, 0] and time_of_frame < time_intervals[time_interval_idx, 1]:\n",
    "                    time_interval_for_patch_idx = time_interval_idx\n",
    "                    time_interval_has_been_found = True\n",
    "                    break\n",
    "                if time_interval_idx == time_intervals.shape[0] - 1 and round(time_of_frame) == time_intervals[time_interval_idx, 1]:\n",
    "                    time_interval_for_patch_idx = time_interval_idx\n",
    "                    time_interval_has_been_found = True\n",
    "                    break\n",
    "            \n",
    "            if time_interval_has_been_found:    \n",
    "                ''' Time interval has been found '''\n",
    "                if probabilities[time_interval_for_patch_idx, undef_class_idx] <= undef_probability_limit:\n",
    "                    ''' Probabilities of this time interval are enough good '''\n",
    "                    ''' Add probabilities of this patch's frame to assembly probabilities '''\n",
    "                    assembly_probabilities = assembly_probabilities + probabilities[time_interval_for_patch_idx, :]\n",
    "                    continue\n",
    "            \n",
    "            ''' Time interval has not been found or probabilities are not good '''\n",
    "            ''' Skip this assembly '''\n",
    "            is_assembly_acceptable = False\n",
    "            break\n",
    "        \n",
    "        ''' if sum of assembly's probabilities are less or equal 0 then they are unacceptable '''\n",
    "        if np.sum(assembly_probabilities) <= 0:\n",
    "            is_assembly_acceptable = False\n",
    "        \n",
    "        if is_assembly_acceptable:\n",
    "                ''' Normalize assembly probabilities '''\n",
    "                assembly_probabilities = assembly_probabilities / np.sum(assembly_probabilities)\n",
    "                ''' Add to learning data '''\n",
    "                learning_assemblies[learning_data_counter, :, :, :] = assemblies[assembly_idx, :, :, :]\n",
    "                learning_probabilities[learning_data_counter, :] = assembly_probabilities\n",
    "                learning_data_counter = learning_data_counter + 1\n",
    "                        \n",
    "        ''' End of cycle '''\n",
    "    \n",
    "    return (learning_assemblies[0 : learning_data_counter, :, :, :], learning_probabilities[0 : learning_data_counter, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Depth parameters '''\n",
    "depth_data_size = 64\n",
    "# Warning: do not forger to set parameter\n",
    "depth_data_parameters = [\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 4],\n",
    "    [2, 2],\n",
    "    [4, 1],\n",
    "    [1, 8],\n",
    "    [2, 4],\n",
    "    [4, 2],\n",
    "    [8, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not forger to set parameter\n",
    "depth_data_parameters_index = 9\n",
    "depth_data_offset = depth_data_parameters[depth_data_parameters_index][0]\n",
    "depth_data_neighbours_per_side = depth_data_parameters[depth_data_parameters_index][1]\n",
    "\n",
    "input_processed_depth_data_directory_path = \"./processed_depth_data/size_\" + str(depth_data_size) + \"_offset_\" + str(depth_data_offset) + \"_left_\" + str(depth_data_neighbours_per_side) + \"_right_\" + str(depth_data_neighbours_per_side) + \"/\"\n",
    "print(input_processed_depth_data_directory_path)\n",
    "\n",
    "output_processed_depth_data_directory_path = \"./learning_data/size_\" + str(depth_data_size) + \"_offset_\" + str(depth_data_offset) + \"_left_\" + str(depth_data_neighbours_per_side) + \"_right_\" + str(depth_data_neighbours_per_side) + \"/\"\n",
    "print(output_processed_depth_data_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not forger to set parameter\n",
    "experiment_idx = 4\n",
    "\n",
    "# Warning: do not forger to write dir path\n",
    "input_processed_laboras_file_directory_path = \"\" + str(experiment_idx) + \".pickle\" \n",
    "print(input_processed_laboras_file_directory_path)\n",
    "\n",
    "''' Load processed laboras data '''\n",
    "time_intervals, probabilities = load_processed_laboras_data(input_processed_laboras_file_directory_path)\n",
    "start_time = np.min(time_intervals[:,0])\n",
    "stop_time = np.max(time_intervals[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: do not forger to set parameter\n",
    "part_idx = 4\n",
    "\n",
    "name = str(experiment_idx) + \"_\" + str(part_idx)\n",
    "\n",
    "input_processed_depth_data_file_path = input_processed_depth_data_directory_path + name + \".pickle\"\n",
    "print(input_processed_depth_data_file_path)\n",
    "\n",
    "output_learning_data_file_path = output_processed_depth_data_directory_path + name + \".pickle\"\n",
    "print(output_learning_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load processed depth data '''\n",
    "number_of_frames, assembly_offsets, assemblies, indecies = load_processed_depth_data(input_processed_depth_data_file_path)\n",
    "\n",
    "if experiment_idx == 2:\n",
    "    for indecies_idx in range(0, indecies.shape[0]):\n",
    "        if indecies[indecies_idx] > number_of_frames:\n",
    "            indecies = indecies[0:indecies_idx]\n",
    "            assemblies = assemblies[0:indecies_idx, :, :, :]\n",
    "            break\n",
    "                \n",
    "if indecies.shape[0] != 0 and assemblies.shape[0] != 0:\n",
    "    ''' Get learning data '''\n",
    "    experiment_learning_assemblies, experiment_learning_probabilities = get_learning_data(start_time,\n",
    "                                                                                          stop_time,\n",
    "                                                                                          time_intervals,\n",
    "                                                                                          probabilities,\n",
    "                                                                                          number_of_frames,\n",
    "                                                                                          indecies,\n",
    "                                                                                          assembly_offsets,\n",
    "                                                                                          assemblies)\n",
    "    write_data_for_learning(output_learning_data_file_path, experiment_learning_assemblies, experiment_learning_probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
